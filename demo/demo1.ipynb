{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c18639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 AI 回應：\n",
      "{'model': 'gemma3:1b', 'created_at': '2025-07-25T22:01:03.916740705Z', 'response': '{\\n\"解釋：\":\"Python的函式就像是程式的指令，你可以把它想像成一個小小的任務。你告訴函式要做什麼，函式就會執行，並產生結果。\"\\n\\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n', 'done': False}\n",
      "{\n",
      "\"解釋：\":\"Python的函式就像是程式的指令，你可以把它想像成一個小小的任務。你告訴函式要做什麼，函式就會執行，並產生結果。\"\n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt: str):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": { #參考說明1\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        },\n",
    "        \"max_tokens\": 100,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    print(\"💬 AI 回應：\")\n",
    "    # Print the whole result for debugging\n",
    "    print(result)\n",
    "    # Try to print the 'response' key if it exists, otherwise print possible keys\n",
    "    if \"response\" in result:\n",
    "        print(result[\"response\"])\n",
    "    elif \"message\" in result:\n",
    "        print(result[\"message\"])\n",
    "    elif \"content\" in result:\n",
    "        print(result[\"content\"])\n",
    "    else:\n",
    "        print(\"No expected key found in response. Available keys:\", result.keys())\n",
    "\n",
    "#範例輸入\n",
    "chat_with_ollama(\"請用簡單的方式解釋什麼是Python的函式？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e79444",
   "metadata": {},
   "source": [
    "### 說明1\n",
    "\n",
    "`options` 物件封裝了對於生成模型行為的三個關鍵調整參數：`temperature`、`top_p` 以及 `top_k`。透過這些設定，我們可以更精細地控制模型在產生文字時的隨機程度與多樣性，以達到更符合需求的輸出風格。\n",
    "\n",
    "`temperature`（溫度）參數設定為 0.7，表示在挑選下一個字元或詞彙時，會根據模型預測機率分佈做溫度縮放。溫度越接近 1，生成結果越隨機、多樣；當溫度降低時，生成更傾向於高機率選擇，輸出結果較為保守且重複性增加。設定為 0.7 能在隨機性與穩定性間取得平衡。\n",
    "\n",
    "`top_p`（又稱 nucleus sampling）設為 0.9，代表每次生成時僅考慮累積機率前 90% 的候選詞彙。換言之，模型先將所有候選依機率由高到低排序，然後從機率總和達到 0.9 的詞彙子集中進行隨機抽樣。這種方法可避免只關注最高機率而忽略其他合理選項，也能自動調整抽樣範圍以抑制極低機率的「噪音」輸出。\n",
    "\n",
    "`top_k` 參數設置為 50，表示在抽樣時僅從預測機率最高的前 50 個詞彙中選擇下一步結果。這是在限制搜索空間大小、提高運算效率與品質控制的常見做法。結合 `top_p` 與 `top_k` 使用，能進一步平衡多樣性與穩定性：`top_k` 確保候選集不超過一定規模，`top_p` 則依實際機率分佈動態修剪集內詞彙。\n",
    "\n",
    "綜合而言，這三項參數共同為生成模型提供了多層次的隨機與篩選機制。依據不同應用場景（如對話系統、文章撰寫或程式碼生成），可微調這些值以獲得更符合需求的結果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "line_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
