{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c18639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 AI 回應：\n",
      "{'model': 'gemma3:1b', 'created_at': '2025-07-25T22:01:03.916740705Z', 'response': '{\\n\"解釋：\":\"Python的函式就像是程式的指令，你可以把它想像成一個小小的任務。你告訴函式要做什麼，函式就會執行，並產生結果。\"\\n\\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n', 'done': False}\n",
      "{\n",
      "\"解釋：\":\"Python的函式就像是程式的指令，你可以把它想像成一個小小的任務。你告訴函式要做什麼，函式就會執行，並產生結果。\"\n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt: str):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": { #參考說明1\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        },\n",
    "        \"max_tokens\": 100,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    print(\"💬 AI 回應：\")\n",
    "    # Print the whole result for debugging\n",
    "    print(result)\n",
    "    # Try to print the 'response' key if it exists, otherwise print possible keys\n",
    "    if \"response\" in result:\n",
    "        print(result[\"response\"])\n",
    "    elif \"message\" in result:\n",
    "        print(result[\"message\"])\n",
    "    elif \"content\" in result:\n",
    "        print(result[\"content\"])\n",
    "    else:\n",
    "        print(\"No expected key found in response. Available keys:\", result.keys())\n",
    "\n",
    "#範例輸入\n",
    "chat_with_ollama(\"請用簡單的方式解釋什麼是Python的函式？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e79444",
   "metadata": {},
   "source": [
    "### 說明1\n",
    "\n",
    "`options` 物件封裝了對於生成模型行為的三個關鍵調整參數：`temperature`、`top_p` 以及 `top_k`。透過這些設定，我們可以更精細地控制模型在產生文字時的隨機程度與多樣性，以達到更符合需求的輸出風格。\n",
    "\n",
    "`temperature`（溫度）參數設定為 0.7，表示在挑選下一個字元或詞彙時，會根據模型預測機率分佈做溫度縮放。溫度越接近 1，生成結果越隨機、多樣；當溫度降低時，生成更傾向於高機率選擇，輸出結果較為保守且重複性增加。設定為 0.7 能在隨機性與穩定性間取得平衡。\n",
    "\n",
    "`top_p`（又稱 nucleus sampling）設為 0.9，代表每次生成時僅考慮累積機率前 90% 的候選詞彙。換言之，模型先將所有候選依機率由高到低排序，然後從機率總和達到 0.9 的詞彙子集中進行隨機抽樣。這種方法可避免只關注最高機率而忽略其他合理選項，也能自動調整抽樣範圍以抑制極低機率的「噪音」輸出。\n",
    "\n",
    "`top_k` 參數設置為 50，表示在抽樣時僅從預測機率最高的前 50 個詞彙中選擇下一步結果。這是在限制搜索空間大小、提高運算效率與品質控制的常見做法。結合 `top_p` 與 `top_k` 使用，能進一步平衡多樣性與穩定性：`top_k` 確保候選集不超過一定規模，`top_p` 則依實際機率分佈動態修剪集內詞彙。\n",
    "\n",
    "綜合而言，這三項參數共同為生成模型提供了多層次的隨機與篩選機制。依據不同應用場景（如對話系統、文章撰寫或程式碼生成），可微調這些值以獲得更符合需求的結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0082f63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "歡迎使用本地端 LLM 聊天機器人（輸入 q 離開）\n",
      "💬 AI 回應：\n",
      "{'model': 'gemma3:1b', 'created_at': '2025-07-25T22:14:27.35581321Z', 'response': '{\\n\"response\": \"天空之所以是藍的，主要是因為一種叫做**瑞利散射 (Rayleigh scattering)** 的現象。 簡單來說，太陽光穿過大氣層時，會與空氣中的微小粒子（主要是氮氣和氧氣）發生碰撞。 由於這些粒子尺寸非常小，碰撞會將太陽光散射到不同的方向。**藍光（波長較短）更容易被散射，因此被散射到天空的每個方向。**\"\\n}', 'done': True, 'done_reason': 'stop', 'context': [105, 2364, 107, 141370, 95202, 237026, 241339, 236918, 106, 107, 105, 4368, 107, 236782, 107, 236775, 6275, 1083, 623, 141370, 132414, 237026, 241339, 236918, 236900, 74836, 27502, 120460, 100083, 1018, 240046, 237766, 150341, 568, 30958, 53700, 19389, 62902, 10363, 121887, 236924, 236743, 74624, 94757, 236900, 83700, 237914, 239409, 238124, 237110, 239471, 239614, 237479, 236900, 238003, 238693, 235450, 12870, 238477, 237369, 166747, 237221, 74836, 243140, 239471, 237206, 240514, 239471, 237214, 90972, 181044, 236924, 236743, 92793, 52426, 166747, 67868, 13869, 237369, 236900, 181044, 238003, 239079, 83700, 237914, 150341, 237238, 35078, 25718, 236924, 1018, 241339, 237914, 237221, 238663, 237953, 239350, 238906, 237214, 186897, 237759, 150341, 236900, 23041, 237759, 150341, 237238, 141370, 236918, 117680, 25718, 236924, 1018, 236775, 107, 236783], 'total_duration': 28573776231, 'load_duration': 336781457, 'prompt_eval_count': 14, 'prompt_eval_duration': 768334282, 'eval_count': 104, 'eval_duration': 26656140121}\n",
      "{\n",
      "\"response\": \"天空之所以是藍的，主要是因為一種叫做**瑞利散射 (Rayleigh scattering)** 的現象。 簡單來說，太陽光穿過大氣層時，會與空氣中的微小粒子（主要是氮氣和氧氣）發生碰撞。 由於這些粒子尺寸非常小，碰撞會將太陽光散射到不同的方向。**藍光（波長較短）更容易被散射，因此被散射到天空的每個方向。**\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def chat_with_ollama(prompt: str):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": { #參考說明1\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        },\n",
    "        \"max_tokens\": 100,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    print(\"💬 AI 回應：\")\n",
    "    # Print the whole result for debugging\n",
    "    print(result)\n",
    "    # Try to print the 'response' key if it exists, otherwise print possible keys\n",
    "    if \"response\" in result:\n",
    "        print(result[\"response\"])\n",
    "    elif \"message\" in result:\n",
    "        print(result[\"message\"])\n",
    "    elif \"content\" in result:\n",
    "        print(result[\"content\"])\n",
    "    else:\n",
    "        print(\"No expected key found in response. Available keys:\", result.keys())\n",
    "\n",
    "    \n",
    "def chat_loop():\n",
    "    print(\"歡迎使用本地端 LLM 聊天機器人（輸入 q 離開）\")\n",
    "    while True:\n",
    "        user_input = input(\"👤 你說：\")\n",
    "        if user_input.lower() == 'q':\n",
    "            break\n",
    "        chat_with_ollama(user_input)\n",
    "\n",
    "chat_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "line_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
